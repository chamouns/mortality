```{r, setup, include=FALSE}
knitr::opts_chunk$set(
  fig.width = 6, fig.height = 6, dpi = 300, cache = TRUE, collapse = TRUE
)
```

# Time-based models
Here we will look at what happens when time-dependent data is added. At the moment, we only use deaths per day per zipcode

## Data wrangling
This is going to be mostly the same as `baseline_models.Rmd`. Refer to that file for more details.
Note that here we're not extracting the AEs.
```{r}
library(tidyverse)
library(tidymodels)
library(feather)
# library(arrow)
library(magrittr)
library(skimr)
library(lubridate)
library(tsibble)
library(fable)
library(feasts)
library(glue)
library(slider)
per <- read_feather("data/simulation_data/all_persons.feather")

clients <-
  per %>%
  group_by(client) %>%
  summarize(
    zip3 = first(zip3),
    size = n(),
    volume = sum(FaceAmt),
    avg_qx = mean(qx),
    avg_age = mean(Age),
    per_male = sum(Sex == "Male") / size,
    per_blue_collar = sum(collar == "blue") / size,
    expected = sum(qx * FaceAmt)
  )

zip_data <-
  read_feather("data/data.feather") %>%
  mutate(
    density = POP / AREALAND,
    AREALAND = NULL,
    AREA = NULL,
    HU = NULL,
    vaccinated = NULL,
    per_lib = NULL,
    per_green = NULL,
    per_other = NULL,
    per_rep = NULL,
    unempl_2020 = NULL,
    deaths_covid = NULL,
    deaths_all = NULL
  ) %>%
  rename(
    unemp = unempl_2019,
    hes_uns = hes_unsure,
    str_hes = strong_hes,
    income = Median_Household_Income_2019
  )

clients %<>%
  inner_join(zip_data, by = "zip3") %>%
  drop_na()
```

Now we have to be careful...

We take daily covid deaths and make them weekly.
Also, look at weekly deaths instead of deaths to date
```{r}
deaths <-
  read_feather("data/deaths_zip3.feather") %>%
  mutate(yw = yearweek(date)) %>%
  select(-date) %>%
  group_by(zip3, yw) %>%
  summarize(totdeaths = max(deaths)) %>%
  mutate(zip_deaths = totdeaths - lag(totdeaths, default = 0)) %>%
  select(-totdeaths) %>%
  ungroup()
```

Let's see what it looks like in Atlanta and LA. We normalize by population.
```{r atl_la_covid}
deaths %>%
  left_join(zip_data, by = "zip3") %>%
  filter(zip3 %in% c("303", "900")) %>%
  ggplot(aes(x = yw, color = zip3)) +
  geom_line(aes(y = zip_deaths / POP))
```

One thing to note, there is no death data for zipcodes 202, 204, 753, 772 (which is fine, since there is no zip_data for those either).

Let's join client data and deaths (maybe not?)
```{r}
# clients %<>%
#   left_join(deaths, by = "zip3") %>%
#   relocate(yw, zip_deaths, .after = "zip3")
```

Next we look at the deaths only
```{r}
per %<>%
  drop_na() %>%
  select(-month)
```

2019 didn't have 53 weeks, so the deaths on week 53 in 2019 will be changed to death in week 1 of 2020.
I tried to do this, but then it turns out that some individuals die many times!!!
Let's fix this
```{r}
per %<>%
  group_by(client, participant) %>%
  arrange(year, week, by_group = TRUE) %>%
  slice_head(n = 1) %>%
  ungroup()
```

Now back to the 53 week business
```{r}
wk53 <-
  per %>%
  filter(year == 2019, week == 53) %>%
  mutate(year = 2020, week = 1)
per %<>%
  rows_update(wk53, by = c("client", "participant"))
```

The tibble `per` contains all the people that die and their deathweek. Everyone should die only once now! Let's convert the week and year into a yearweek object
```{r}
per %<>%
  nest_by(week, year) %>%
  mutate(yw = yearweek(glue("{year} w{week}")), .before = "week") %>%
  ungroup() %>%
  select(-week, -year) %>%
  unnest(cols = c(data))
```



Next, we need some kind of a rolling count for AE. Looks like the package `slider` might help.
I want actual claims per week for each client.
We note that there are 4 clients that won't have any deaths
```{r}
no_deaths <-
  clients %>%
  anti_join(per, by = "client") %>%
  select(client) %>%
  mutate(yw = yearweek("2019 w01"), claims = 0)
```

We compute face amount per week for each client. This number is 0 if the client has no deaths that week.
```{r}
weekly_claims <-
  per %>%
  group_by(yw, client) %>%
  summarize(claims = sum(FaceAmt), .groups = "drop") %>%
  bind_rows(no_deaths) %>%
  complete(yw, client, fill = list(claims = 0))
```

We now have 65,369 rows, which is 131 weeks * 499 clients.

Let's merge everything.
```{r}
weekly_data <-
  clients %>%
  left_join(weekly_claims, by = c("client")) %>%
  relocate(yw) %>%
  relocate(claims, .after = zip3) %>%
  left_join(deaths, by = c("yw", "zip3")) %>%
  relocate(zip_deaths, .after = claims) %>%
  mutate(zip_deaths = replace_na(zip_deaths, 0))
```

We add a rolling AE number. This will be the AE for the 26 week prior to that week.
TODO: Early 2019 is a bit broken, we should remove it.
```{r}
weekly_data %<>%
  group_by(client) %>%
  mutate(rolling_ae = slide_index_dbl(claims, yw, sum, .before = 52, .complete = TRUE) / (first(expected) / 1)) %>%
  relocate(rolling_ae, .before = size) %>%
  ungroup() %>%
  drop_na()
```

Let's see if this makes sense. We plot the rolling AE and deaths in the zip code for client 7.
```{r}
weekly_data %>%
  filter(client == "7") %>%
  ggplot(aes(x = yw)) + geom_line(aes(y = rolling_ae)) + geom_line(aes(y = zip_deaths), color = "red")
```

Cool. Let's make it into a `tsibble` (time series tibble)
```{r}
weekly_data %<>%
  as_tsibble(key = client, index = yw)
```



## Some models
We will throw some models at this data. We will start with a random forest.
We will split the data with respect to time.
We train on the past times.
To predict, we first need to forecast `weekly_deaths`, and then use the model.

Here is one way of forecasting with the `fable` package.
```{r}
weekly_data %>%
  filter(client == 2, yw < yearweek("2021 w1")) %>%
  model(arima = ARIMA(zip_deaths)) %>%
  forecast(h = "3 months") %>%
  autoplot(filter(weekly_data, year(yw) > 2019))
```

We will preprocess the data a little bit.
```{r}
forest_data <-
  weekly_data %>%
  mutate(class = fct_recode(factor(rolling_ae < 3), adverse = "FALSE", `not adverse` = "TRUE"), .after = yw) %>%
  select(-rolling_ae)
```

Pick a training and validation set
```{r}
start <- yearweek("2020 w26")
lookback <- 52
end <- 26
training <-
  forest_data %>%
  filter(yw <= start & yw > start - lookback)

testing <-
  forest_data %>%
  filter(yw > start & yw <= start + end)
```



First let's do a random forest with default parameters
```{r}
forest_spec <-
  rand_forest(trees = 1000) %>%
  set_engine("ranger", num.threads = 8, seed = 123456789) %>%
  set_mode("classification")

forest_recipe <-
  recipe(class ~ ., data = training) %>%
  # step_rm(client, zip3, yw, claims) %>%
  step_rm(client, zip3, claims) %>%
  step_zv(all_predictors())

forest_wf <-
  workflow() %>%
  add_recipe(forest_recipe) %>%
  add_model(forest_spec)

forest_fit <-
  forest_wf %>%
  fit(training)
```

Now we estimate deaths for the next 6 weeks
```{r}
forecasted_deaths <-
  training %>%
  model(
    # arima = ARIMA(zip_deaths ~ pdq() + PDQ(0,0,0)),
    arima = ARIMA(zip_deaths),
    # ets = ETS(zip_deaths ~ trend("A"))
    # tslm = TSLM(zip_deaths ~ trend())
  ) %>%
  forecast(h = "6 months")

forecasted_testing <-
  forecasted_deaths %>%
  as_tibble() %>%
  select(client, yw, .mean) %>%
  rename(zip_deaths = .mean) %>%
  right_join( testing %>% select(-zip_deaths) )

forest_fit %>%
  predict(forecasted_testing, type = "prob") %>%
  bind_cols(forecasted_testing) %>%
  group_by(yw) %>%
  summarize(roc_auc = roc_auc_vec(class, .pred_adverse)) %>%
  ggplot(aes(x = yw, y = roc_auc)) + geom_point() + geom_line()
```

Let's use the forecasts from IHME.
... TODO


Creating splits for each week
```{r}
forest_data %>%
  arrange(yw) %>%
  sliding_index(yw, lookback = 12, assess_stop = 26)
```


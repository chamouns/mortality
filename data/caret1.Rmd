```{r}
  library(tidyverse)
  library(feather)
  library(caret)
```

Read simulate data
```{r}
  experience <- readRDS("simulation_data/experience_weekly_1.RDS")
  person <-readRDS("simulation_data/person_1.RDS")
```


k-month ae value for 2019. for any k, around 10% client's ae is over the threshold 1.1.
Here we choose k = 1 for the following work.
```{r}
  k = 1
  test2019 <- experience %>%
    filter(year == 2019)%>%
    left_join(person, by = c("client", "participant"))%>%
    select(-zip3, -industry, -Age, -collar, -Sex, -year)%>%
    mutate(kmonth = (month-1) %/% k)%>%
    group_by(client, participant, kmonth)%>%
    summarise(actual = sum(death) * FaceAmt, expect = sum(qx) * FaceAmt) %>%
    ungroup()%>%
    group_by(client, kmonth)%>%
    summarise(actual = sum(actual), expect = sum(expect)) %>%
    mutate(ae = actual / expect)%>%
    mutate(adverse = ae >1.1)
```
```{r}
  summary(test2019$adverse)
```

delete the test data in case we don't have enough memory
```{r}
  test2019<-NULL
```


calculate the k-month ae value
```{r}
  k = 1
  actual_ae <- experience %>%
    left_join(person, by = c("client", "participant"))%>%
    select(-industry, -Age, -collar, -Sex)%>%
    #mutate(kmonth = (month-1) %/% k)%>%
    group_by(client, participant,zip3,  month, year)%>%
    summarise(actual = sum(death) * FaceAmt, expect = sum(qx) * FaceAmt) %>%
    ungroup()%>%
    group_by(client, zip3, year, month)%>%
    summarise(actual = sum(actual), expect = sum(expect)) %>%
    mutate(ae = actual / expect)%>%
    mutate(adverse = ae >1.1)
```

Since covid start spread in 2020, the more and more client's ae values are above the threshod 1.1.
We need to bulid a new model to provide with better expected claim to manage the risk in the future pandemic situation.

```{r}
  summary(actual_ae$adverse[actual_ae$year==2019])
  summary(actual_ae$adverse[actual_ae$year==2020])
  summary(actual_ae$adverse[actual_ae$year==2021])
```


```{r}
  unique(person$client)#list the clients
  length(unique(person$client))# number of the clients
```

Build model:

Here we choose the first three month data as the train data (2020.1-2020.3)
```{r}
try1 <- actual_ae%>%
    filter(year == 2020, month == c(1,2,3))
```

I choose this one because there is detail example online.
L
-LightGBM: 
```{r}
library(data.table)
library(Matrix)
library(dplyr)
library(MLmetrics)
library(lightgbm)
```

-Split data
     -Simple Splitting Based on the Outcome
```{r}
set.seed(3456)
trainIndex <- createDataPartition(try1$adverse, p = .8, 
                                  list = FALSE, 
                                  times = 1)
```
```{r}
Train <- try1[ trainIndex,]
Test <- try1[-trainIndex,]
```
Pre-processing
```{r}
median.impute = function(x){
  x = as.data.frame(x)
  for (i in 1:ncol(x)){
    x[which(x[,i]== -1),i] = NA
  }
  
  x = x %>% mutate_all(~ifelse(is.na(.), median(., na.rm = TRUE), .)) %>% as.data.table()
  return(x)
}

Train = median.impute(Train)
Test  = median.impute(Test)
```
Feature Engineering
```{r}
Test$ae = NA
data = rbind(Train, Test)

data[, fe_amount_NA := rowSums(data == -1, na.rm = T)]
```

Create LGB Dataset
```{r}
varnames = setdiff(colnames(data), c("client", "ae"))
train_sparse = Matrix(as.matrix(data[!is.na(ae), varnames, with=F]), sparse=TRUE)
test_sparse  = Matrix(as.matrix(data[is.na(ae) , varnames, with=F]), sparse=TRUE)

y_train  = data[!is.na(ae),ae]
test_ids = data[is.na(ae) ,client]

lgb.train = lgb.Dataset(data=train_sparse, label=y_train)

categoricals.vec = colnames(train)[c(grep("cat",colnames(train)))]
```

Setting up LGBM Parameters
```{r}

lgb.grid = list(objective = "binary",
                metric = "auc",
                min_sum_hessian_in_leaf = 1,
                feature_fraction = 0.7,
                bagging_fraction = 0.7,
                bagging_freq = 5,
                min_data = 100,
                max_bin = 50,
                lambda_l1 = 8,
                lambda_l2 = 1.3,
                min_data_in_bin=100,
                min_gain_to_split = 10,
                min_data_in_leaf = 30,
                is_unbalance = TRUE)
```

Setting up Gini Eval Function
```{r}
# Gini for Lgb
lgb.normalizedgini = function(preds, dtrain){
  actual = getinfo(dtrain, "label")
  score  = NormalizedGini(preds,actual)
  return(list(name = "gini", value = score, higher_better = TRUE))
}
```
Train Final Model
```{r}
best.iter = 525
# Train final model
lgb.model = lgb.train(params = lgb.grid, data = lgb.train, learning_rate = 0.02,
                      num_leaves = 25, num_threads = 2 , nrounds = best.iter,
                      eval_freq = 20, eval = lgb.normalizedgini,
                      categorical_feature = categoricals.vec)
```

```{r}
preds = data.table(id=test_ids, target=predict(lgb.model,test_sparse))
colnames(preds)[1] = "id"
fwrite(preds, "submission.csv")
```













